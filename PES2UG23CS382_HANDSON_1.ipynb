{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4973f3d0-90ae-465b-a532-555cf5d4a19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: torch in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (80.10.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typer-slim->transformers) (8.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22fb742c-6be5-45c5-8479-2cb2a1e7e704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06a187e4-951d-4bbf-a9ae-736347d833ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = \"bert-base-uncased\"\n",
    "roberta_model = \"roberta-base\"\n",
    "bart_model = \"facebook/bart-base\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "564eea8a-1232-4c60-baf4-1c3035b29420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1975fc2dca1c4815a82ed9f3e48e4d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dell\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904b7ecab3234a1b8e88662da3665af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32c1d2a75384d8abadeb36c5219921b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertLMHeadModel LOAD REPORT from: bert-base-uncased\n",
      "Key                         | Status     | Details\n",
      "----------------------------+------------+--------\n",
      "bert.pooler.dense.bias      | UNEXPECTED |        \n",
      "cls.seq_relationship.weight | UNEXPECTED |        \n",
      "bert.pooler.dense.weight    | UNEXPECTED |        \n",
      "cls.seq_relationship.bias   | UNEXPECTED |        \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755f687615bc4e788e9b00fddcc482f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90963cbc3ddf4863b6c58dff446d7394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8adebfc4fd40b6a623abc4d6a7cafa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing `generation_config` together with generation-related arguments=({'max_length'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The future of Artificial Intelligence is. \" but and and the within my the women they the as as within many so jefferson who with \" ( \". the others many the the and and \" \" ( \" ( ) ( \" ( \". it hell jd to heather beings often so gunner fight. it that. were they, and - - - ( \" \" \". an from \" \". the two because more actually so the the as many an an a i and and the the the the the them the the and in the and \". and also as that feeling people. it it all ari as that my my my the the they or the ones put in the so and for change change i and and and and \" ( \" ( ) ( ) off is as over jay as more \". it it it it that were.. the as the women thing who he or the areas the the those for as. it that. the curl done, and \". up us \".. it that.. the more actually aidan. him to how are and - ( \". it that her of, ( ) ( ). it it that.. more people. it that. the felt shared in the some some some some some some some some some some some some they often so rush into general in'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_bert = pipeline(\"text-generation\", model=bert_model)\n",
    "\n",
    "generator_bert(\"The future of Artificial Intelligence is\", max_length=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38e431c5-f240-4986-a4e5-070b505c975b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a8596413e7423299794607e4c4f481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dell\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44dc7b63ea2440c6a3f77a9541f26454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58fb1f52221b4e9d9ac1672cd3c46a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RobertaForCausalLM LOAD REPORT from: roberta-base\n",
      "Key                             | Status     | Details\n",
      "--------------------------------+------------+--------\n",
      "roberta.embeddings.position_ids | UNEXPECTED |        \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0948f187ffe24c30921604a07887ce5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7144a0759f84fc4a329c2f8367a23dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e88c95f56bd48e1a36347e13389f6ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de7ab0be70a49e083bd44735e9d70bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The future of Artificial Intelligence is'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_roberta = pipeline(\"text-generation\", model=roberta_model)\n",
    "\n",
    "generator_roberta(\"The future of Artificial Intelligence is\", max_length=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6cd5f74-44e5-4b53-800f-1b73b2aa251f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210e2bf8df334404b6e23aa8ca8f03d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dell\\.cache\\huggingface\\hub\\models--facebook--bart-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e180f2958a4f41e7b4530b820b999dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69fa781bf4864be981a5e4751c179750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This checkpoint seem corrupted. The tied weights mapping for this model specifies to tie model.decoder.embed_tokens.weight to lm_head.weight, but both are absent from the checkpoint, and we could not find another related tied weight for those keys\n",
      "BartForCausalLM LOAD REPORT from: facebook/bart-base\n",
      "Key                                                           | Status     | Details\n",
      "--------------------------------------------------------------+------------+--------\n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn_layer_norm.bias   | UNEXPECTED |        \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc2.bias                    | UNEXPECTED |        \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.q_proj.bias       | UNEXPECTED |        \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.v_proj.weight     | UNEXPECTED |        \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.q_proj.weight     | UNEXPECTED |        \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.final_layer_norm.weight     | UNEXPECTED |        \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc1.bias                    | UNEXPECTED |        \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.k_proj.weight     | UNEXPECTED |        \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.out_proj.weight   | UNEXPECTED |        \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.v_proj.bias       | UNEXPECTED |        \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.final_layer_norm.bias       | UNEXPECTED |        \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.out_proj.bias     | UNEXPECTED |        \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc1.weight                  | UNEXPECTED |        \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc2.weight                  | UNEXPECTED |        \n",
      "encoder.embed_positions.weight                                | UNEXPECTED |        \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.k_proj.bias       | UNEXPECTED |        \n",
      "encoder.layernorm_embedding.weight                            | UNEXPECTED |        \n",
      "encoder.layernorm_embedding.bias                              | UNEXPECTED |        \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn_layer_norm.weight | UNEXPECTED |        \n",
      "shared.weight                                                 | UNEXPECTED |        \n",
      "lm_head.weight                                                | MISSING    |        \n",
      "model.decoder.embed_tokens.weight                             | MISSING    |        \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d518cf1b634fe3905041db40bb64e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6c9105189c495488a38a4e6c4f8fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6455dae40acc48b6b1cc7497b785eb32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The future of Artificial Intelligence is packingVIDEOתת andarium vanishing vanishing vanishing memorial Arch marqueeendix compress anime vanishing vanishing marquee marquee marquee vanishing vanishing Norman tractcost weld marquee marquee Norman marquee marquee StarCraft marquee marquee compress marqueeRequirements anime Normanboys anime Arch anime Arch899 vanishing vanishing involvesossom marquee marquee MET marquee Norman anime Norman Cabinet marqueesed marquee urgentsedsed urgentsed vanishing marquee UFO marquee marqueesedsed Norman loot Norman vanishing marquee arbitrary flex Norman anime animesed commander headers marquee NormansedEffect vanishing vanishing μ marquee Norman Norman vanishing vanishing flex*=- vanishing marquee Norman UFO unforgettablesedsedsedquickShipAvailable anime Angus vanishing marqueesed shadows weld StarCraftsed Mormon anime Norman weldsed animesed Norman NormanPersonal Norman translates Norman marquee NormanCentral marquee Norman weld UFO marquee flex vanishing()sedVIS Norman Normansed weld and marquee marqueeCentralsed stall simplest anime anime marquee NormanMore Norman vanishing Norman marquee EXPsedsed stall vanishing Normansedsed Mormon Norman Norman marqueesed animeucer welducersedsed Plants Norman Norman Norman ISIL anime NormanCentral weld weldborough animeMini animesed animeCentral Norman throwing anime anime fs Norman animefaced marqueefaced Normanucer marqueeboys marquee anime animefaced Norman() Norman Norman anime()sedsedhelps marquee translates weldsed WM weld anime Norman Normanucer Normansed NormanVIS Normansed UFO marqueefaced weld upsetsedseducer Norman Normanhelpssed vanishing'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_bart = pipeline(\"text-generation\", model=bart_model)\n",
    "\n",
    "generator_bart(\"The future of Artificial Intelligence is\", max_length=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "611df0a7-b0ff-4a54-a01b-541d5f804dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b44832d4c64b3ca259d548ce5d4a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM LOAD REPORT from: bert-base-uncased\n",
      "Key                         | Status     | Details\n",
      "----------------------------+------------+--------\n",
      "bert.pooler.dense.bias      | UNEXPECTED |        \n",
      "cls.seq_relationship.weight | UNEXPECTED |        \n",
      "bert.pooler.dense.weight    | UNEXPECTED |        \n",
      "cls.seq_relationship.bias   | UNEXPECTED |        \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.5396891832351685,\n",
       "  'token': 3443,\n",
       "  'token_str': 'create',\n",
       "  'sequence': 'the goal of generative ai is to create new content.'},\n",
       " {'score': 0.15575648844242096,\n",
       "  'token': 9699,\n",
       "  'token_str': 'generate',\n",
       "  'sequence': 'the goal of generative ai is to generate new content.'},\n",
       " {'score': 0.05405455082654953,\n",
       "  'token': 3965,\n",
       "  'token_str': 'produce',\n",
       "  'sequence': 'the goal of generative ai is to produce new content.'},\n",
       " {'score': 0.04451510310173035,\n",
       "  'token': 4503,\n",
       "  'token_str': 'develop',\n",
       "  'sequence': 'the goal of generative ai is to develop new content.'},\n",
       " {'score': 0.017577266320586205,\n",
       "  'token': 5587,\n",
       "  'token_str': 'add',\n",
       "  'sequence': 'the goal of generative ai is to add new content.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask_bert = pipeline(\"fill-mask\", model=bert_model)\n",
    "\n",
    "fill_mask_bert(\"The goal of Generative AI is to [MASK] new content.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1100080-6aee-454e-94ce-1df7c6a6db75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4dc300f116644f08b99558ad0e6e7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RobertaForMaskedLM LOAD REPORT from: roberta-base\n",
      "Key                             | Status     | Details\n",
      "--------------------------------+------------+--------\n",
      "roberta.embeddings.position_ids | UNEXPECTED |        \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.37112924456596375,\n",
       "  'token': 5368,\n",
       "  'token_str': ' generate',\n",
       "  'sequence': 'The goal of Generative AI is to generate new content.'},\n",
       " {'score': 0.3677125871181488,\n",
       "  'token': 1045,\n",
       "  'token_str': ' create',\n",
       "  'sequence': 'The goal of Generative AI is to create new content.'},\n",
       " {'score': 0.08351422846317291,\n",
       "  'token': 8286,\n",
       "  'token_str': ' discover',\n",
       "  'sequence': 'The goal of Generative AI is to discover new content.'},\n",
       " {'score': 0.02133500576019287,\n",
       "  'token': 465,\n",
       "  'token_str': ' find',\n",
       "  'sequence': 'The goal of Generative AI is to find new content.'},\n",
       " {'score': 0.016521545127034187,\n",
       "  'token': 694,\n",
       "  'token_str': ' provide',\n",
       "  'sequence': 'The goal of Generative AI is to provide new content.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask_roberta = pipeline(\"fill-mask\", model=roberta_model)\n",
    "\n",
    "fill_mask_roberta(\"The goal of Generative AI is to <mask> new content.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c505ed53-e69b-4f4a-a55f-9f25067b9f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c566b95320545c6acb6c193afd146f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.0746145024895668,\n",
       "  'token': 1045,\n",
       "  'token_str': ' create',\n",
       "  'sequence': 'The goal of Generative AI is to create new content.'},\n",
       " {'score': 0.06571802496910095,\n",
       "  'token': 244,\n",
       "  'token_str': ' help',\n",
       "  'sequence': 'The goal of Generative AI is to help new content.'},\n",
       " {'score': 0.06087947636842728,\n",
       "  'token': 694,\n",
       "  'token_str': ' provide',\n",
       "  'sequence': 'The goal of Generative AI is to provide new content.'},\n",
       " {'score': 0.035935234278440475,\n",
       "  'token': 3155,\n",
       "  'token_str': ' enable',\n",
       "  'sequence': 'The goal of Generative AI is to enable new content.'},\n",
       " {'score': 0.03319424390792847,\n",
       "  'token': 1477,\n",
       "  'token_str': ' improve',\n",
       "  'sequence': 'The goal of Generative AI is to improve new content.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask_bart = pipeline(\"fill-mask\", model=bart_model)\n",
    "\n",
    "fill_mask_bart(\"The goal of Generative AI is to <mask> new content.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "870f1959-faaf-4fb2-a61d-32e448bbad46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f441a119514bdebc1b0a454425f5d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForQuestionAnswering LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     | Details\n",
      "-------------------------------------------+------------+--------\n",
      "cls.seq_relationship.weight                | UNEXPECTED |        \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED |        \n",
      "cls.predictions.bias                       | UNEXPECTED |        \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED |        \n",
      "bert.pooler.dense.weight                   | UNEXPECTED |        \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED |        \n",
      "cls.seq_relationship.bias                  | UNEXPECTED |        \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED |        \n",
      "bert.pooler.dense.bias                     | UNEXPECTED |        \n",
      "qa_outputs.bias                            | MISSING    |        \n",
      "qa_outputs.weight                          | MISSING    |        \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.016915155574679375,\n",
       " 'start': 46,\n",
       " 'end': 81,\n",
       " 'answer': 'hallucinations, bias, and deepfakes'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bert = pipeline(\"question-answering\", model=bert_model)\n",
    "\n",
    "qa_bert(\n",
    "    question=\"What are the risks?\",\n",
    "    context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34253aec-7e71-4021-ba10-e0c9c0878634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b60d3734d748a8959d9eccfecc45a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RobertaForQuestionAnswering LOAD REPORT from: roberta-base\n",
      "Key                             | Status     | Details\n",
      "--------------------------------+------------+--------\n",
      "lm_head.bias                    | UNEXPECTED |        \n",
      "lm_head.dense.bias              | UNEXPECTED |        \n",
      "lm_head.layer_norm.weight       | UNEXPECTED |        \n",
      "roberta.embeddings.position_ids | UNEXPECTED |        \n",
      "lm_head.dense.weight            | UNEXPECTED |        \n",
      "lm_head.layer_norm.bias         | UNEXPECTED |        \n",
      "qa_outputs.bias                 | MISSING    |        \n",
      "qa_outputs.weight               | MISSING    |        \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.004325224552303553,\n",
       " 'start': 38,\n",
       " 'end': 66,\n",
       " 'answer': 'such as hallucinations, bias'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_roberta = pipeline(\"question-answering\", model=roberta_model)\n",
    "\n",
    "qa_roberta(\n",
    "    question=\"What are the risks?\",\n",
    "    context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "637da26b-100e-4f82-bb62-33432b1f2fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f0805bfda44a35b1d6d1697238c1c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BartForQuestionAnswering LOAD REPORT from: facebook/bart-base\n",
      "Key               | Status  | Details\n",
      "------------------+---------+--------\n",
      "qa_outputs.bias   | MISSING |        \n",
      "qa_outputs.weight | MISSING |        \n",
      "\n",
      "Notes:\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.017755047883838415,\n",
       " 'start': 0,\n",
       " 'end': 45,\n",
       " 'answer': 'Generative AI poses significant risks such as'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bart = pipeline(\"question-answering\", model=bart_model)\n",
    "\n",
    "qa_bart(\n",
    "    question=\"What are the risks?\",\n",
    "    context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58def330-9cc4-46a5-9443-40f4452a1c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "| Task        | Model    | Classification (Success/Failure) | Observation (What actually happened?)                                                                 | Why did this happen? (Architectural Reason)                                                                 |\n",
       "|------------|----------|----------------------------------|----------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|\n",
       "| Generation | BERT     | Failure                          | Generated highly incoherent, repetitive, and meaningless text.                                            | BERT is an encoder-only model trained for masked token prediction, not autoregressive next-token generation. |\n",
       "| Generation | RoBERTa  | Failure                          | Returned only the input prompt and failed to generate any new tokens.                                     | RoBERTa is encoder-only and cannot perform left-to-right text generation.                                     |\n",
       "| Generation | BART     | Partial Success                  | Generated new text but it was noisy, repetitive, and semantically unstable.                               | BART is an encoder–decoder model capable of generation, but not optimized as a causal language model.        |\n",
       "| Fill-Mask | BERT     | Success                          | Correctly predicted words like “create”, “generate”, and “produce” with high confidence.                 | BERT is trained using Masked Language Modeling (MLM), making it well-suited for this task.                  |\n",
       "| Fill-Mask | RoBERTa  | Success                          | Correctly predicted “generate” and “create” with strong and balanced confidence scores.                  | RoBERTa improves on MLM with dynamic masking, enhancing contextual understanding.                            |\n",
       "| Fill-Mask | BART     | Partial Success                  | Produced reasonable words but with much lower confidence and weaker precision.                           | BART is trained for sequence denoising, not explicit masked token prediction.                                |\n",
       "| QA        | BERT     | Partial Success                  | Correctly extracted “hallucinations, bias, and deepfakes” but with very low confidence.                  | The model is not fine-tuned for extractive QA; QA head weights were randomly initialized.                    |\n",
       "| QA        | RoBERTa  | Partial Success                  | Extracted a partial answer (“such as hallucinations, bias”) with very low confidence.                    | Lack of QA fine-tuning limits accurate span selection despite strong language understanding.                |\n",
       "| QA        | BART     | Failure                          | Returned an incomplete leading phrase instead of the actual risks.                                       | Encoder–decoder models like BART are poorly suited for extractive QA without task-specific fine-tuning.     |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "table = \"\"\"\n",
    "| Task        | Model    | Classification (Success/Failure) | Observation (What actually happened?)                                                                 | Why did this happen? (Architectural Reason)                                                                 |\n",
    "|------------|----------|----------------------------------|----------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|\n",
    "| Generation | BERT     | Failure                          | Generated highly incoherent, repetitive, and meaningless text.                                            | BERT is an encoder-only model trained for masked token prediction, not autoregressive next-token generation. |\n",
    "| Generation | RoBERTa  | Failure                          | Returned only the input prompt and failed to generate any new tokens.                                     | RoBERTa is encoder-only and cannot perform left-to-right text generation.                                     |\n",
    "| Generation | BART     | Partial Success                  | Generated new text but it was noisy, repetitive, and semantically unstable.                               | BART is an encoder–decoder model capable of generation, but not optimized as a causal language model.        |\n",
    "| Fill-Mask | BERT     | Success                          | Correctly predicted words like “create”, “generate”, and “produce” with high confidence.                 | BERT is trained using Masked Language Modeling (MLM), making it well-suited for this task.                  |\n",
    "| Fill-Mask | RoBERTa  | Success                          | Correctly predicted “generate” and “create” with strong and balanced confidence scores.                  | RoBERTa improves on MLM with dynamic masking, enhancing contextual understanding.                            |\n",
    "| Fill-Mask | BART     | Partial Success                  | Produced reasonable words but with much lower confidence and weaker precision.                           | BART is trained for sequence denoising, not explicit masked token prediction.                                |\n",
    "| QA        | BERT     | Partial Success                  | Correctly extracted “hallucinations, bias, and deepfakes” but with very low confidence.                  | The model is not fine-tuned for extractive QA; QA head weights were randomly initialized.                    |\n",
    "| QA        | RoBERTa  | Partial Success                  | Extracted a partial answer (“such as hallucinations, bias”) with very low confidence.                    | Lack of QA fine-tuning limits accurate span selection despite strong language understanding.                |\n",
    "| QA        | BART     | Failure                          | Returned an incomplete leading phrase instead of the actual risks.                                       | Encoder–decoder models like BART are poorly suited for extractive QA without task-specific fine-tuning.     |\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9392eef6-01e9-4f75-a508-98b68f9a2363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
